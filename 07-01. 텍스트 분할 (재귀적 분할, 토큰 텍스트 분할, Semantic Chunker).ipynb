{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9fa2af1",
   "metadata": {},
   "source": [
    "# 텍스트 분할(Text Splitter)\n",
    "- 문서분할은 Retrieval-Augmented Generation(RAG) 시스템의 두 번째 단계로서, 로드된 문서들을 효율적으로 처리하고, 시스템이 정보를 보다 잘 활용할 수 있도록 준비하는 중요한 과정\n",
    "\n",
    "    **크고 복잡한 문서를 LLM 이 받아들일 수 있는 효율적인 작은 규모의 조각으로 나누는 작업이며, 나중에 사용자가 입력한 질문에 대하여 보다 효율적인 정보만 압축/선별하여 가져오기 위함**\n",
    "\n",
    "<br>\n",
    "\n",
    "### 분할의 필요성\n",
    "- **핀포인트 정보 검색(정확성)**: 문서를 세분화함으로써 질문(Query) 에 연관성이 있는 정보만 가져오는데 도움\n",
    "  - 각각의 단위는 특정 주제나 내용에 초점을 맞추므로, 관련성이 높은 정보를 제공\n",
    "- **리소스 최적화(효율성)**: 전체 문서를 LLM 으로 입력하게 되면 비용이 많이 발생할 뿐더러, 효율적인 답변을 많은 정보속에 발췌하여 답변하지 못하게 됨\n",
    "  - 이러한 문제가 할루시네이션으로 이어지게 되며, 답변에 필요한 정보만 발췌하기 위한 목적\n",
    "\n",
    "<br>\n",
    "\n",
    "### 문서분할 과정\n",
    "1. **문서 구조 파악**: PDF 파일, 웹 페이지, 전자 책 등 다양한 형식의 문서에서 구조를 파악. 이는 문서의 헤더, 푸터, 페이지 번호, 섹션 제목 등을 식별하는 과정을 포함\n",
    "2. **단위 선정**: 문서를 어떤 단위로 나눌지 결정. 이는 페이지별, 섹션별, 또는 문단별일 수 있으며, 문서의 내용과 목적에 따라 다름\n",
    "3. **단위 크기 선정(chunk size)**: 문서를 몇 개의 토큰 단위로 나눌 것인지를 결정\n",
    "4. **청크 오버랩(chunk overlap)**: 분할된 끝 부분에서 맥락이 이어질 수 있도록 일부를 겹쳐서(overlap) 분할하는 것이 일반적\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 문자 텍스트 분할(`CharacterTextSplitter`)\n",
    "\n",
    "<br>\n",
    "\n",
    "### `CharacterTextSplitter`\n",
    "- 가장 간단한 방식\n",
    "- `\"\\n\\n\"` 을 기준으로 문자 단위로 텍스트를 분할하고, 청크의 크기를 문자 수로 측정\n",
    "    - **텍스트 분할 방식**: **단일 문자** 기준\n",
    "    - **청크 크기 측정 방식**: **문자 수** 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d92cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3f11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08a609",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 텍스트를 청크(chunk)로 분할\n",
    "- `separator` : 매개변수로 분할할 기준을 설정. 기본 값은 `\"\\n\\n\"`\n",
    "- `chunk_size` : 각 청크의 최대 크기를 제한\n",
    "- `chunk_overlap` : 인접한 청크 간에 몇 자의 중복을 허용할지 설정\n",
    "- `length_function` : 텍스트의 길이를 계산하는 함수를 지정\n",
    "- `is_separator_regex` : `separator`를 정규식이 아닌 일반 문자열로 처리 $\\rightarrow$ `False`\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08276f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bde8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    # 텍스트를 분할할 때 사용할 구분자를 지정. 기본값은 \"\\n\\n\"\n",
    "    separator=\"\\n\\n\",\n",
    "    \n",
    "    # 분할된 텍스트 청크의 최대 크기를 지정\n",
    "    chunk_size=250,\n",
    "    \n",
    "    # 분할된 텍스트 청크 간의 중복되는 문자 수를 지정\n",
    "    chunk_overlap=50,\n",
    "    \n",
    "    # 텍스트의 길이를 계산하는 함수를 지정\n",
    "    length_function=len,\n",
    "    \n",
    "    # 구분자가 정규식인지 여부를 지정\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302d6e7",
   "metadata": {},
   "source": [
    "- file 텍스트를 문서 단위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81697b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb2f92",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 메타데이터가 문서와 함께 분할\n",
    "- `create_documents` 메서드는 텍스트 데이터와 메타데이터 리스트를 인자롭 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42a492f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "metadatas = [\n",
    "    {\"document\": 1},\n",
    "    {\"document\": 2},\n",
    "]  # 문서에 대한 메타데이터 리스트를 정의\n",
    "\n",
    "documents = text_splitter.create_documents(\n",
    "    [\n",
    "        file,\n",
    "        file,\n",
    "    ],  # 분할할 텍스트 데이터를 리스트로 전달\n",
    "    metadatas=metadatas,  # 각 문서에 해당하는 메타데이터를 전달\n",
    ")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81736803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4cd6eb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 재귀적 문자 텍스트 분할(`RecursiveCharacterTextSplitter`)\n",
    "- **일반적인 텍스트에 권장되는 방식**\n",
    "- **문자 목록을 매개변수로 받아 동작 $\\rightarrow$ 분할기는 청크가 충분히 작아질 때까지 주어진 문자 목록의 순서대로 텍스트를 분할하려고 시도**\n",
    "  - 기본 문자 목록은 `[\"\\n\\n\", \"\\n\", \" \", \"\"]`\n",
    "\n",
    "<br>\n",
    "\n",
    "- **단락 $\\rightarrow$ 문장 $\\rightarrow$ 단어 순서로 재귀적으로 분할**\n",
    "  - 단락(그 다음으로 문장, 단어) 단위가 의미적으로 가장 강하게 연관된 텍스트 조각으로 간주되므로, 가능한 한 함께 유지하려는 효과\n",
    "  - **텍스트가 분할되는 방식**: 문자 목록(`[\"\\n\\n\", \"\\n\", \" \", \"\"]`) 에 의해 분할\n",
    "  - **청크 크기가 측정되는 방식**: 문자 수에 의해 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb04bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2deb7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### `RecursiveCharacterTextSplitter`\n",
    "- `chunk_size` : 250 으로 설정하여 각 청크의 크기를 제한\n",
    "- `chunk_overlap` : 50 으로 설정하여 인접한 청크 간에 50 개 문자의 중첩을 허용\n",
    "- `length_function` : len 함수를 사용하여 텍스트의 길이를 계산\n",
    "- `is_separator_regex` : `False`로 설정하여 구분자로 정규식을 사용하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "778a18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ba3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a140039d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n",
      "============================================================\n",
      "page_content='Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([file])\n",
    "\n",
    "print(texts[0])\n",
    "print(\"===\" * 20)\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576c938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding',\n",
       " 'Embedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(file)[:2]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b422316",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 토큰 텍스트 분할(`TokenTextSplitter`)\n",
    "- 언어 모델에는 토큰 제한 $\\rightarrow$ 토큰 제한을 초과하지 않아야 함\n",
    "  \n",
    "  `TokenTextSplitter` 는 텍스트를 토큰 수를 기반으로 청크를 생성할 때 유용\n",
    "\n",
    "<br>\n",
    "\n",
    "### `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cefa3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae845ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed87abd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `from_tiktoken_encoder` 메서드를 사용하여 `Tiktoken` 인코더 기반의 텍스트 분할기를 초기화\n",
    "  - `CharacterTextSplitter.from_tiktoken_encoder`를 사용할 경우, 텍스트는 `CharacterTextSplitter`에 의해서만 분할되고 `tiktoken` 토크나이저는 분할된 텍스트를 병합하는 데 사용\n",
    "    \n",
    "    $\\rightarrow$ **분할된 텍스트가 `tiktoken` 토크나이저로 측정한 청크 크기보다 클 수 있음**\n",
    "\n",
    "  - `RecursiveCharacterTextSplitter.from_tiktoken_encoder`를 사용할 경우, 분할된 텍스트가 언어 모델에서 허용하는 토큰의 청크 크기보다 크지 않도록 할 수 있으며, 각 분할은 크기가 더 큰 경우 재귀적으로 분할\n",
    "    \n",
    "    $\\rightarrow$ **각 분할이 청크 크기보다 작음을 보장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aefda1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8d5ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # 청크 크기를 300으로 설정\n",
    "    chunk_size=300,\n",
    "    \n",
    "    # 청크 간 중복되는 부분이 없도록 설정\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d729e3e",
   "metadata": {},
   "source": [
    "- file 텍스트를 청크 단위로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4754926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "Semantic Search\n"
     ]
    }
   ],
   "source": [
    "print(len(texts)) # 분할된 청크의 개수\n",
    "print(texts[0]) # texts 리스트의 첫 번째 요소를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ca4da",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `TokenTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8765249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08e0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,  # 청크 간 중복을 0으로 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d215a736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"�\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b7505",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `spaCy`\n",
    "\n",
    "```bash\n",
    "$ pip install --upgrade --quiet spacy\n",
    "$ python -m spacy download en_core_web_sm --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abdd5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b535128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8759dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0466850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된\n",
      "\n",
      "결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. \n",
      "\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나\n",
      "\n",
      "문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다.\n",
      "\n",
      "이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "\n",
      "\n",
      "예시: \"사과\"라는 단어를\n",
      "\n",
      "[0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0], '\\n')\n",
    "print(texts[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211ea00",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### NLTK    \n",
    "\n",
    "- 단순히 `\"\\n\\n\"`으로 분할하는 대신, NLTK tokenizers를 기반으로 텍스트를 분할\n",
    "\n",
    "```bash\n",
    "$ pip install -qU nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de8fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f26ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42135e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5c8beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c51dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 시멘틱 청커(SemanticChunker)\n",
    "* `SemanticChunker`는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dafd32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8018ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "141f8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6d28e",
   "metadata": {},
   "source": [
    "- OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da446d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bef03e",
   "metadata": {},
   "source": [
    "- 텍스트 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01dc3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(file)\n",
    "print(chunks[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99158add",
   "metadata": {},
   "source": [
    "- 청크를 문서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c2339dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea8ee8d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Breakpoints\n",
    "- 문장을 \"분리\"할 시점을 결정하여 작동 **(두 문장 간의 임베딩 차이)**\n",
    "\n",
    "    $\\rightarrow$ 그 차이가 특정 임계값을 넘으면 문장이 분리\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Percentile\n",
    "- **기본적인 분리 방식은 백분위수(`Percentile`) 를 기반**\n",
    "- 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52297d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    \n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화\n",
    "    OpenAIEmbeddings(),\n",
    "    \n",
    "    # 분할 기준점 유형을 백분위수로 설정\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85590ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================ \n",
      "\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "============================================================ \n",
      "\n",
      "[Chunk 2]\n",
      "\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================ \n",
      "\n",
      "[Chunk 3]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================ \n",
      "\n",
      "[Chunk 4]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(\"===\" * 20, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2001de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35d81b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Standard Deviation\n",
    "- **지정한 `breakpoint_threshold_amount` 표준편차보다 큰 차이가 있는 경우 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0b08d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    \n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화\n",
    "    OpenAIEmbeddings(),\n",
    "    \n",
    "    # 분할 기준으로 표준 편차를 사용\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbcceef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc957b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Interquartile\n",
    "- 사분위수 범위(interquartile range)를 사용하여 청크를 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b77464d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    \n",
    "    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화\n",
    "    OpenAIEmbeddings(),\n",
    "    \n",
    "    # 분할 기준점 임계값 유형을 사분위수 범위로 설정\n",
    "    breakpoint_threshold_type=\"interquartile\",\n",
    "    breakpoint_threshold_amount=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa226e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531134a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "lang_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
