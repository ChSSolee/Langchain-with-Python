{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a0f1cb",
   "metadata": {},
   "source": [
    "# 검색기 (Retriever)\n",
    "- **검색기(Retriever) 단계는 Retrieval-Augmented Generation(RAG) 시스템의 다섯 번째 단계로, 저장된 벡터 데이터베이스에서 사용자의 질문과 관련된 문서를 검색하는 과정**\n",
    "- **사용자 질문에 가장 적합한 정보를 신속하게 찾아내는 것이 목표이며, RAG 시스템의 전반적인 성능과 직결되는 매우 중요한 과정**\n",
    "\n",
    "<br>\n",
    "\n",
    "### 검색기의 필요성\n",
    "- **정확한 정보 제공**: 검색기는 사용자의 질문과 가장 관련성 높은 정보를 검색하여, 시스템이 정확하고 유용한 답변을 생성할 수 있도록 함\n",
    "  - 이 과정이 효과적으로 이루어지지 않으면, 결과적으로 제공되는 답변의 품질이 떨어질 수 있음\n",
    "- **응답 시간 단축**: 효율적인 검색 알고리즘을 사용하여 데이터베이스에서 적절한 정보를 빠르게 검색함으로써, 전체적인 시스템 응답 시간을 단축\n",
    "- **최적화**: 효과적인 검색 과정을 통해 필요한 정보만을 추출함으로써 시스템 자원의 사용을 최적화하고, 불필요한 데이터 처리를 줄일 수 있음\n",
    "\n",
    "<br>\n",
    "\n",
    "### 동작 방식\n",
    "- **질문의 벡터화**: 사용자의 질문을 벡터 형태로 변환\n",
    "  - 이 과정은 임베딩 단계와 유사한 기술을 사용하여 진행되며, 변환된 질문 벡터는 후속 검색 작업의 기준점으로 사용\n",
    "- **벡터 유사성 비교**: 저장된 문서 벡터들과 질문 벡터 사이의 유사성을 계산. \n",
    "  - **주로 코사인 유사성(cosine similarity), Max Marginal Relevance(MMR) 등의 수학적 방법을 사용하여 수행**\n",
    "- **상위 문서 선정**: 계산된 유사성 점수를 기준으로 상위 N개의 가장 관련성 높은 문서를 선정. \n",
    "  - 이 문서들은 다음 단계에서 사용자의 질문에 대한 답변을 생성하는 데 사용\n",
    "- **문서 정보 반환**: 선정된 문서들의 정보를 다음 단계(프롬프트 생성)로 전달. \n",
    "  \n",
    "  (이 정보에는 문서의 내용, 위치, 메타데이터 등이 포함)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 검색기의 중요성\n",
    "- 검색기는 RAG 시스템에서 정보 검색의 질을 결정하는 핵심적인 역할 \n",
    "- 효율적인 검색기 없이는 대규모 데이터베이스에서 관련 정보를 신속하고 정확하게 찾아내는 것이 매우 어려움\n",
    "- **또한, 검색기는 사용자 질문에 대한 적절한 컨텍스트를 제공하여, 언어 모델이 보다 정확한 답변을 생성할 수 있도록 도움**\n",
    "  \n",
    "  $\\rightarrow$ 검색기의 성능은 RAG 시스템의 전반적인 효율성과 사용자 만족도에 직접적인 영향\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sparse Retriever & Dense Retriever\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Sparse Retriever\n",
    "- Sparse Retriever는 문서와 쿼리(질문)를 **이산적인 키워드 벡터**로 변환하여 처리\n",
    "  - **TF-IDF** (Term Frequency-Inverse Document Frequency): 단어가 문서에 나타나는 빈도와 그 단어가 몇 개의 문서에서 나타나는지를 반영하여 단어의 중요도를 계산\n",
    "    - 여기서, 자주 나타나면서도 문서 집합 전체에서 드물게 나타나는 단어가 높은 가중치를 받음\n",
    "  - **BM25**: TF-IDF를 개선한 모델로, 문서의 길이를 고려하여 검색 정확도를 향상. 긴 문서와 짧은 문서 간의 가중치를 조정하여, 단어 빈도의 영향을 상대적으로 조절\n",
    "  \n",
    "<br>\n",
    "\n",
    "- Sparse Retriever의 특징은 각 단어의 존재 여부만을 고려하기 때문에 계산 비용이 낮고, 구현이 간단\n",
    "  \n",
    "  $\\rightarrow$ 그러나 이 방법은 단어의 의미적 연관성을 고려하지 않으며, 검색 결과의 품질이 키워드의 선택에 크게 의존\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Dense Retriever\n",
    "- Dense Retriever는 최신 딥러닝 기법을 사용하여 문서와 쿼리를 연속적인 고차원 벡터로 인코딩\n",
    "- **Dense Retriever는 문서의 의미적 내용을 보다 풍부하게 표현**할 수 있으며, 키워드가 완벽히 일치하지 않더라도 의미적으로 관련된 문서를 검색\n",
    "- **Dense Retriever는 벡터 공간에서의 거리(예: 코사인 유사도)를 사용**하여 쿼리와 가장 관련성 높은 문서를 탐색\n",
    "\n",
    "  $\\rightarrow$ 특히 언어의 뉘앙스와 문맥을 이해하는 데 유리하며, 복잡한 쿼리에 대해 더 정확한 검색 결과를 제공\n",
    "\n",
    "<br>\n",
    "\n",
    "### 차이점\n",
    "- **표현 방식**: Sparse Retriever는 이산적인 키워드 기반의 표현을 사용하는 반면, Dense Retriever는 연속적인 벡터 공간에서 의미적 표현을 사용\n",
    "- **의미적 처리 능력**: **Dense Retriever는 문맥과 의미를 더 깊이 파악**할 수 있어, 키워드가 정확히 일치하지 않아도 관련 문서를 검색. \n",
    "  \n",
    "  **Sparse Retriever는 이러한 의미적 뉘앙스를 덜 반영**\n",
    "- **적용 범위**: **복잡한 질문이나 자연어 쿼리에 대해서는 Dense Retriever가 더 적합할 수 있으며, 간단하고 명확한 키워드 검색에는 Sparse Retriever가 더 유용**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed1ad17",
   "metadata": {},
   "source": [
    "## 벡터스토어 기반 검색기(VectorStore-backed Retriever)\n",
    "- VectorStore 지원 검색기 는 **vector store를 사용하여 문서를 검색하는 retriever**\n",
    "- Vector store에 구현된 유사도 검색(similarity search) 이나 MMR 과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3f3f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46fc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3307a9",
   "metadata": {},
   "source": [
    "- 문서 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff19842",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/appendix-keywords.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687294cc",
   "metadata": {},
   "source": [
    "- 텍스트 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc627a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4eda8e",
   "metadata": {},
   "source": [
    "- 임베더 활용 벡터스토어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244dd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c742d1d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### VectorStore에서 VectorStoreRetriever 초기화\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `as_retriever`\n",
    "-  VectorStore 객체를 기반으로 VectorStoreRetriever를 초기화하고 반환\n",
    "-  `**kwargs`: 검색 함수에 전달할 키워드 인자\n",
    "-  `search_type`: 검색 유형 (\"similarity\", \"mmr\", \"similarity_score_threshold\")\n",
    "   -  `search_kwargs`: 추가 검색 옵션\n",
    "   -  `k`: 반환할 문서 수 (기본값: 4)\n",
    "   -  `score_threshold`: similarity_score_threshold 검색의 최소 유사도 임계값\n",
    "   -  `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "   -  `lambda_mult`: MMR 결과의 다양성 조절 (0-1 사이, 기본값: 0.5)\n",
    "   -  `filter`: 문서 메타데이터 기반 필터링\n",
    "\n",
    "<br>\n",
    "\n",
    "- `search_type`과 `search_kwargs의` 적절한 조합 필요\n",
    "- MMR 사용 시 `fetch_k`와 `k` 값의 균형 조절 필요\n",
    "- `score_threshold` 설정 시 너무 높은 값은 검색 결과가 없을 수 있음\n",
    "- 필터 사용 시 데이터셋의 메타데이터 구조 정확히 파악 필요\n",
    "- `lambda_mult` 값이 0에 가까울수록 다양성이 높아지고, 1에 가까울수록 유사성이 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d91f383",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### `invoke`\n",
    "*  Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용. 이 메서드는 동기적으로 Retriever를 호출하여 주어진 쿼리에 대한 관련 문서를 반환\n",
    "- `input`: 검색 쿼리 문자열\n",
    "- `config`: Retriever 구성 (Optional[RunnableConfig])\n",
    "- `**kwargs`: Retriever에 전달할 추가 인자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b9c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9a887",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Max Marginal Relevance (MMR)\n",
    "- 쿼리에 대한 관련 항목을 검색할 때 검색된 문서의 중복 을 피하는 방법 중 하나\n",
    "- **단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 쿼리에 대한 문서의 관련성 과 이미 선택된 문서들과의 차별성을 동시에 고려**\n",
    "- `search_type` 매개변수를 \"mmr\" 로 설정하여 MMR(Maximal Marginal Relevance) 검색 알고리즘을 사용\n",
    "- `k`: 반환할 문서 수 (기본값: 4)\n",
    "- `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "- `lambda_mult`: MMR 결과의 다양성 조절 (0~1, 기본값: 0.5, 0: 유사도 점수만 고려, 1: 다양성만 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0741d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f2a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbfef5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 유사도 점수 임계값 검색(similarity_score_threshold)\n",
    "- **유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법**\n",
    "\n",
    "    $\\rightarrow$ **임계값을 적절히 설정함으로써 관련성이 낮은 문서를 필터링 하고, 질의와 가장 유사한 문서만 선별**\n",
    "- search_type 매개변수를 \"`similarity_score_threshold`\" 로 설정하여 유사도 점수 임계값을 기준으로 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8bbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf7c964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "for doc in retriever.invoke(\"Word2Vec 은 무엇인가요?\"):\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f22ada",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### `top_k` 설정\n",
    "- `k`매개변수는 검색 결과에서 반환할 상위 결과의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d114b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b26b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63da80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 동적 설정(Configurable)\n",
    "- 검색 설정을 동적으로 조정하기 위해 `ConfigurableField` 를 사용\n",
    "  - `ConfigurableField` 는 검색 매개변수의 고유 식별자, 이름, 설명을 설정하는 역할\n",
    "- 검색 설정을 조정하기 위해 `config` 매개변수를 사용하여 검색 설정을 지정\n",
    "- 검색 설정은 `config` 매개변수에 전달된 딕셔너리의 `configurable` 키에 저장\n",
    "- 검색 설정은 검색 쿼리와 함께 전달되며, 검색 쿼리에 따라 동적으로 조정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b265b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bff2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
    "    search_type=ConfigurableField(\n",
    "        id=\"search_type\",\n",
    "        name=\"Search Type\",\n",
    "        description=\"The search type to use\",\n",
    "    ),\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",\n",
    "        name=\"Search Kwargs\",\n",
    "        description=\"The search kwargs to use\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323b7cc",
   "metadata": {},
   "source": [
    "- Faiss 검색에서 k=3로 설정하여 가장 유사한 문서 3개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\", config=config)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88955be5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `score_threshold` 0.8 이상의 점수를 가진 문서만 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b403354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"similarity_score_threshold\",\n",
    "        \"search_kwargs\": {\n",
    "            \"score_threshold\": 0.8,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66103df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- mmr 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0feeb2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"search_kwargs\": {\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6},\n",
    "    }\n",
    "}\n",
    "\n",
    "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f42f4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 문맥 압축 검색기(ContextualCompressionRetriever)\n",
    "- 검색 시스템에서 직면하는 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 특정 질의를 처리해야 할지 미리 알 수 없다는 점\n",
    "  \n",
    "  $\\rightarrow$ **질의와 가장 관련성이 높은 정보가, 많은 양의 무관한 텍스트를 포함한 문서에 묻혀 있을 수 있음**\n",
    "\n",
    "  $\\rightarrow$ **전체 문서를 애플리케이션에 전달하면 더 비용이 많이 드는 LLM 호출과 품질이 낮은 응답으로 이어짐**\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `ContextualCompressionRetriever `\n",
    "- **검색된 문서를 그대로 즉시 반환하는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환**\n",
    "  \n",
    "  **\"압축\"은 개별 문서의 내용을 압축하는 것과 문서를 전체적으로 필터링하는 것 모두를 의미**\n",
    "- 질의를 base `retriever`에 전달하고, 초기 문서를 가져와 Document Compressor를 통과\n",
    "\n",
    "<img src='https://wikidocs.net/images/page/234097/01-Contextual-Compression.jpeg' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b1c88",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e45247cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1173053a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 기본 Retriever 설정\n",
    "- `retriever는` 관련 있는 문서 1~2개와 관련 없는 문서 몇 개를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2aeaeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc38da8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/appendix-keywords.txt\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "texts = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51946288",
   "metadata": {},
   "source": [
    "- FAISS 벡터 저장소를 생성하고 검색기로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b240de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "064e3d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      "\n",
      "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
      "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
      "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
      "\n",
      "Page Rank\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 3:\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 4:\n",
      "\n",
      "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
      "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
      "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
      "\n",
      "데이터 마이닝\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
    "\n",
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35b49e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 맥락적 압축(ContextualCompression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f20441e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d9203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68d70f",
   "metadata": {},
   "source": [
    "- 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2acb7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c58d20dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = (\n",
    "    compression_retriever.invoke(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n",
    "        \"Semantic Search 에 대해서 알려줘.\"\n",
    "    )\n",
    ")\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bfb50",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LLM 을 활용한 문서 필터링\n",
    "- 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반환할지 결정하기 위해 LLM 체인을 사용하는 압축기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150180f2",
   "metadata": {},
   "source": [
    "- LLMChainFilter 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d3fca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_filter = LLMChainFilter.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "139f022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter,\n",
    "    base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad676ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c05e42",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### EmbeddingsFilter\n",
    "- 각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느림\n",
    "  \n",
    "  $\\rightarrow$ **`EmbeddingsFilter`는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠른 옵션을 제공**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 지정된 유사도 임계값(0.86) 이상인 문서를 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b02726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "906560f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff55de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, base_retriever=retriever\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    # 쿼리\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fdf77",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 파이프라인 생성(압축기+문서 변환기)\n",
    "- 여러 compressor를 순차적으로 결합\n",
    "- `BaseDocumentTransformer` : 맥락적 압축을 수행하지 않고 단순히 문서 집합에 대한 변환을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b683cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b93702",
   "metadata": {},
   "source": [
    "- 문자 기반 텍스트 분할기를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c9e41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50695817",
   "metadata": {},
   "source": [
    "- 임베딩을 사용하여 중복 필터를 생성\n",
    "- 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a90ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799a405",
   "metadata": {},
   "source": [
    "- 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터, `LLMChainExtractor`를 변환기로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d2e52d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[\n",
    "        splitter,\n",
    "        redundant_filter,\n",
    "        relevant_filter,\n",
    "        LLMChainExtractor.from_llm(llm),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8692b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,\n",
    "    base_retriever=retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb1439e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
     ]
    }
   ],
   "source": [
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search 에 대해서 알려줘.\"\n",
    ")\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ec8f8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 앙상블 검색기(EnsembleRetriever)\n",
    "- **여러 검색기를 결합**하여 더 강력한 검색 결과를 제공하는 LangChain의 기능\n",
    "\n",
    "    $\\rightarrow$ 다양한 검색 알고리즘의 장점을 활용하여 단일 알고리즘보다 더 나은 성능을 달성\n",
    "\n",
    "<br>\n",
    "\n",
    "- **여러 검색기 통합**: 다양한 유형의 검색기를 입력으로 받아 결과를 결합\n",
    "- **결과 재순위화**: [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) 알고리즘을 사용하여 결과의 순위를 조정\n",
    "- **하이브리드 검색**: 주로 sparse retriever(예: BM25)와 dense retriever(예: 임베딩 유사도)를 결합하여 사용\n",
    "  - Sparse retriever: 키워드 기반 검색에 효과적 \n",
    "  - Dense retriever: 의미적 유사성 기반 검색에 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b46e6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e986bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "    \"I like apples\",\n",
    "    \"I like apple company\",\n",
    "    \"I like apple's iphone\",\n",
    "    \"Apple is my favorite company\",\n",
    "    \"I like apple's ipad\",\n",
    "    \"I like apple's macbook\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee28a2d",
   "metadata": {},
   "source": [
    "- bm25 retriever와 faiss retriever를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e3dc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list,\n",
    ")\n",
    "bm25_retriever.k = 1\n",
    "\n",
    "embedding = OpenAIEmbeddings() \n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list,\n",
    "    embedding,\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcfb0b",
   "metadata": {},
   "source": [
    "- 앙상블 retriever를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d90822e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.7, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee008a03",
   "metadata": {},
   "source": [
    "- 검색 결과 문서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble Retriever]\n",
      "Content: Apple is my favorite company\n",
      "\n",
      "Content: I like apple's iphone\n",
      "\n",
      "[BM25 Retriever]\n",
      "Content: Apple is my favorite company\n",
      "\n",
      "[FAISS Retriever]\n",
      "Content: I like apple's iphone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Apple company makes my favorite iphone\"\n",
    "ensemble_result = ensemble_retriever.invoke(query)\n",
    "bm25_result = bm25_retriever.invoke(query)\n",
    "faiss_result = faiss_retriever.invoke(query)\n",
    "\n",
    "print(\"[Ensemble Retriever]\")\n",
    "for doc in ensemble_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[BM25 Retriever]\")\n",
    "for doc in bm25_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[FAISS Retriever]\")\n",
    "for doc in faiss_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690969f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 런타임 Config 변경\n",
    "- 런타임에서도 retriever 의 속성을 `ConfigurableField` 클래스를 사용하여 변경\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb316426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de50b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(\n",
    "    # 리트리버 목록을 설정\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    \n",
    ").configurable_fields(\n",
    "    weights=ConfigurableField(\n",
    "        # 검색 매개변수의 고유 식별자를 설정합니다.\n",
    "        id=\"ensemble_weights\",\n",
    "        # 검색 매개변수의 이름을 설정합니다.\n",
    "        name=\"Ensemble Weights\",\n",
    "        # 검색 매개변수에 대한 설명을 작성합니다.\n",
    "        description=\"Ensemble Weights\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27189ed6",
   "metadata": {},
   "source": [
    "- 검색 시 `config` 매개변수를 통해 검색 설정을 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12859541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Apple is my favorite company'),\n",
       " Document(id='b3f88d75-caf0-4588-9175-0b0cc1dcf8b4', metadata={}, page_content='I like apples')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"ensemble_weights\": [1, 0]}}\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cef6ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b3f88d75-caf0-4588-9175-0b0cc1dcf8b4', metadata={}, page_content='I like apples'),\n",
       " Document(metadata={}, page_content='Apple is my favorite company')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"ensemble_weights\": [0, 1]}}\n",
    "\n",
    "docs = ensemble_retriever.invoke(\"my favorite fruit is apple\", config=config)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffa368",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc2f7",
   "metadata": {},
   "source": [
    "### 긴 문맥 재정렬(LongContextReorder)\n",
    "- 모델의 아키텍처와 상관없이, 10개 이상의 검색된 문서를 포함할 경우 성능이 상당히 저하\n",
    "  \n",
    "  **모델이 긴 컨텍스트 중간에 있는 관련 정보에 접근해야 할 때, 제공된 문서를 무시하는 경향**\n",
    "\n",
    "  $\\rightarrow$ **검색 후 문서의 순서를 재배열하여 성능 저하를 방지**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d5bbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "368b5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3f90c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"이건 그냥 내가 아무렇게나 적어본 글입니다.\",\n",
    "    \"사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.\",\n",
    "    \"아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.\",\n",
    "    \"챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.\",\n",
    "    \"챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.\",\n",
    "    \"애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.\",\n",
    "    \"ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.\",\n",
    "    \"비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.\",\n",
    "    \"ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.\",\n",
    "    \"FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6a35ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "081d0bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.'),\n",
       " Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'),\n",
       " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
       " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
       " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
       " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
       " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
       " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
       " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"ChatGPT에 대해 무엇을 말해줄 수 있나요?\"\n",
    "\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f88863",
   "metadata": {},
   "source": [
    "- **문서 재정렬 : 덜 관련된 문서는 목록의 중간에 위치하고 더 관련된 요소는 시작/끝에 위치**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c48df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='ChatGPT의 기능은 지속적인 학습과 업데이트를 통해 더욱 발전하고 있습니다.'),\n",
       " Document(metadata={}, page_content='챗GPT는 OpenAI에 의해 개발되었으며, 지속적으로 개선되고 있습니다.'),\n",
       " Document(metadata={}, page_content='챗지피티는 사용자의 질문을 이해하고 적절한 답변을 생성하기 위해 대량의 데이터를 학습했습니다.'),\n",
       " Document(metadata={}, page_content='아이폰, 아이패드, 맥북 등은 애플이 출시한 대표적인 제품들입니다.'),\n",
       " Document(metadata={}, page_content='FIFA 월드컵은 네 번째 해마다 열리며, 국제 축구에서 가장 큰 행사입니다.'),\n",
       " Document(metadata={}, page_content='애플 워치와 에어팟 같은 웨어러블 기기도 애플의 인기 제품군에 속합니다.'),\n",
       " Document(metadata={}, page_content='비트코인은 디지털 금이라고도 불리며, 가치 저장 수단으로서 인기를 얻고 있습니다.'),\n",
       " Document(metadata={}, page_content='이건 그냥 내가 아무렇게나 적어본 글입니다.'),\n",
       " Document(metadata={}, page_content='사용자와 대화하는 것처럼 설계된 AI인 ChatGPT는 다양한 질문에 답할 수 있습니다.'),\n",
       " Document(metadata={}, page_content='ChatGPT는 복잡한 문제를 해결하거나 창의적인 아이디어를 제안하는 데에도 사용될 수 있습니다.')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs) # 문서 목록 docs를 재정렬\n",
    "\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a228f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbf992",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 상위 문서 검색기(ParentDocumentRetriever)\n",
    "- 문서 검색 과정에서 문서를 적절한 크기의 조각(청크)으로 나누는 것은 중요한 요소를 고려\n",
    "\n",
    "    **문서가 너무 길면 임베딩이 의미를 잃어버림**\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `ParentDocumentRetriever`\n",
    "- 문서를 작은 조각으로 나누고, 이 조각들을 관리\n",
    "- **검색을 진행할 때는, 먼저 이 작은 조각들을 찾아낸 다음, 이 조각들이 속한 원본 문서(또는 더 큰 조각)의 식별자(ID)를 통해 전체적인 맥락을 파악**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **문서 간의 계층 구조 활용**: `ParentDocumentRetriever`는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용\n",
    "- **검색 성능 향상**: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 탐색. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4581ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24783164",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [TextLoader(\"data/appendix-keywords.txt\")]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fb830e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 전체 문서 검색\n",
    "- 자식 분할기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae3a5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "store = InMemoryStore()\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a5ebd0",
   "metadata": {},
   "source": [
    "- 문서목록을 추가\n",
    "  - `add_to_docstore=False` 로 설정시 document 를 중복으로 추가하지 않음\n",
    "    \n",
    "    **(단, 중복을 체크하기 위한 ids 값이 필수 값으로 요구)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab04efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.add_documents(docs, ids=None, add_to_docstore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "759493d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['71f165d3-e074-47c3-b64a-3c901240c9a1']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683ef1c",
   "metadata": {},
   "source": [
    "- **유사도 검색** : 작은 청크(chunk)들을 저장하고 있기 때문에, 검색 결과로 작은 청크들이 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a71b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
    "\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "269a40fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 길이: 5733\n",
      "\n",
      "=====================\n",
      "\n",
      " 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
      "연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "\n",
      "정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Word2Vec\")\n",
    "\n",
    "print(\n",
    "    f\"문서의 길이: {len(retrieved_docs[0].page_content)}\",\n",
    "    end=\"\\n\\n=====================\\n\\n\",\n",
    ")\n",
    "\n",
    "print(retrieved_docs[0].page_content[2000:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca50cd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 더 큰 Chunk 의 크기를 조절\n",
    "- 전체 문서가 너무 커서 있는 그대로 검색하기에는 부적합 할 경우 \n",
    "  \n",
    "  $\\rightarrow$ **원시 문서를 더 큰 청크로 분할한 다음, 더 작은 청크로 분할**\n",
    "\n",
    "  $\\rightarrow$ **그런 다음 작은 청크들을 인덱싱하지만, 검색 시에는 더 큰 청크를 검색**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- 부모 문서를 생성하는 데 사용되는 텍스트 분할기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1892a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "\n",
    "# 부모 문서의 저장 계층\n",
    "store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6080c7",
   "metadata": {},
   "source": [
    "- 자식 문서를 생성하는 데 사용되는 텍스트 분할기\n",
    "  - 부모보다 작은 문서를 생성   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d5eea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817f303",
   "metadata": {},
   "source": [
    "- 자식 청크를 인덱싱하는 데 사용할 벡터 저장소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78f3de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0191d7a",
   "metadata": {},
   "source": [
    "- 리트리버"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cca4b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    # 벡터 저장소를 지정합\n",
    "    vectorstore=vectorstore,\n",
    "    \n",
    "    # 문서 저장소를 지정\n",
    "    docstore=store,\n",
    "    \n",
    "    # 하위 문서 분할기를 지정\n",
    "    child_splitter=child_splitter,\n",
    "    \n",
    "    # 상위 문서 분할기를 지정\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011d2b4",
   "metadata": {},
   "source": [
    "- 문서들을 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "def8e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_documents(docs)  \n",
    "\n",
    "len(list(store.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e884e2a",
   "metadata": {},
   "source": [
    "- 벡터 스토어에서 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b1e0146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n"
     ]
    }
   ],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
    "\n",
    "print(sub_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a99843",
   "metadata": {},
   "source": [
    "- 리트리버에서 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae98bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n",
      "예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\n",
      "연관키워드: 딥러닝, 자연어 처리, Attention\n",
      "\n",
      "HuggingFace\n",
      "\n",
      "정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n",
      "예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\n",
      "연관키워드: 자연어 처리, 딥러닝, 라이브러리\n",
      "\n",
      "Digital Transformation\n",
      "\n",
      "정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n",
      "예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\n",
      "연관키워드: 혁신, 기술, 비즈니스 모델\n",
      "\n",
      "Crawling\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"Word2Vec\")\n",
    "\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db1f56",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "lang_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
