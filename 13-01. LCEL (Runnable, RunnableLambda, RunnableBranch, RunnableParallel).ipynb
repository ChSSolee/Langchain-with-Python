{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6249cd5",
   "metadata": {},
   "source": [
    "# LCEL\n",
    "- LangChain Expression Language (LCEL)은 LangChain 라이브러리에서 제공하는 선언적 방식의 인터페이스로, \n",
    "  \n",
    "  복잡한 LLM (Large Language Model) 애플리케이션을 구축하고 실행하기 위한 도구\n",
    "\n",
    "* LCEL은 LLM, 프롬프트, 검색기, 메모리 등 다양한 컴포넌트를 조합하여 강력하고 유연한 AI 시스템을 구축 가능\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 주요 특징\n",
    "- **선언적 구문**: 복잡한 로직을 간결하고 읽기 쉬운 방식으로 표현\n",
    "- **모듈성**: 다양한 컴포넌트를 쉽게 조합하고 재사용\n",
    "- **유연성**: 다양한 유형의 LLM 애플리케이션을 구축\n",
    "- **확장성**: 사용자 정의 컴포넌트를 쉽게 통합\n",
    "- **최적화**: 실행 시 자동으로 최적화를 수행\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## RunnablePassthrough\n",
    "- `invoke()` 메서드를 통해 입력된 데이터를 그대로 반환\n",
    "-  다음과 같은 시나리오에서 유용\n",
    "   - 데이터를 변환하거나 수정할 필요가 없는 경우\n",
    "   - 파이프라인의 특정 단계를 건너뛰어야 하는 경우\n",
    "   - 디버깅 또는 테스트 목적으로 데이터 흐름을 모니터링해야 하는 경우\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Runnable = 입력 $\\rightarrow$ 출력 변환을 수행하는, 체이닝 가능한 추상 연산자**\n",
    "  - 어떤 입력(Input)을 받아서, 어떤 출력(Output)을 반환할 수 있는 실행 가능한 단위\n",
    "  - LCEL 전체는 Runnable 들을 조합해 만든 실행 그래프\n",
    "\n",
    "| LCEL 객체             | Runnable로서의 역할             |\n",
    "| ------------------- | -------------------------- |\n",
    "| `PromptTemplate`      | 입력 dict → 포맷된 문자열 변환       |\n",
    "| `ChatModel`           | 문자열 → LLM 응답 반환            |\n",
    "| `Retriever`           | query → relevant documents |\n",
    "| `OutputParser`        | LLM 결과 → Python 객체로 변환     |\n",
    "| `RunnableLambda`      | 아무 Python 함수도 Runnable화    |\n",
    "| `RunnableParallel`    | 병렬 구동 Runnable             |\n",
    "| `RunnableMap`         | 여러 key별 runnable 실행        |\n",
    "| `RunnablePassthrough` | 입력을 그대로 전달                 |\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### 데이터 전달\n",
    "- `RunnablePassthrough` 는 입력을 변경하지 않고 그대로 전달하거나 추가 키를 더하여 전달\n",
    "  - `assign`과 함께 호출된 `RunnablePassthrough(RunnablePassthrough.assign(...))`는 입력을 받아 `assign` 함수에 전달된 추가 인자를 더함\n",
    "- `RunnableParallel` 클래스를 사용하여 병렬로 실행 가능한 작업을 정의\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22a5bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a742a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c15feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    # 전달된 입력을 그대로 반환하는 Runnable\n",
    "    passed=RunnablePassthrough(),\n",
    "    # 입력의 \"num\" 값에 3을 곱한 결과를 반환하는 Runnable\n",
    "    extra=RunnablePassthrough.assign(mult=lambda x: x[\"num\"] * 3),\n",
    "    # 입력의 \"num\" 값에 1을 더한 결과를 반환하는 Runnable\n",
    "    modified=lambda x: x[\"num\"] + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b36c96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {\"num\": 1}을 입력으로 Runnable을 실행\n",
    "runnable.invoke({\"num\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967db6dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 검색기 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5668d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac82057",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\n",
    "        \"테디는 랭체인 주식회사에서 근무를 하였습니다.\",\n",
    "        \"셜리는 테디와 같은 회사에서 근무하였습니다.\",\n",
    "        \"테디의 직업은 개발자입니다.\",\n",
    "        \"셜리의 직업은 디자이너입니다.\",\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2298c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2685d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da6bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef99e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10478fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테디의 직업은 개발자입니다.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"테디의 직업은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d873ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'셜리의 직업은 디자이너입니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"셜리의 직업은 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9d21d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906fdc78",
   "metadata": {},
   "source": [
    "## Runnable 구조(그래프) 검토\n",
    "- LCEL로 `runnable`을 생성한 후에는 이를 검사하여 어떤일이 일어나고 있는지 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df99b3d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 텍스트 데이터로부터 FAISS 벡터 저장소를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7490e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Teddy is an AI engineer who loves programming!\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1704c",
   "metadata": {},
   "source": [
    "- 벡터 저장소를 기반으로 `retriever`를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e557bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}  \n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ffd4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fc4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747ea05",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 그래프 구성 확인\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `chain.get_graph()`\n",
    "- 체인의 각 노드와 노드 간의 연결을 나타내는 그래프 객체를 반환\n",
    "- 그래프의 노드는 체인의 각 단계를 나타내며, `edge`는 단계 간의 데이터 흐름을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d79c1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 체인의 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fdc3fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3cf3be516b214f25bd651cc71d5fbf6d': Node(id='3cf3be516b214f25bd651cc71d5fbf6d', name='Parallel<context,question>Input', data=<class 'langchain_core.runnables.base.RunnableParallel<context,question>Input'>, metadata=None),\n",
       " 'f7272a26361b4909a58253463a9c7ca3': Node(id='f7272a26361b4909a58253463a9c7ca3', name='Parallel<context,question>Output', data=<class 'langchain_core.utils.pydantic.RunnableParallel<context,question>Output'>, metadata=None),\n",
       " 'f5ff3b2b857843e4ad149a5f0f206b54': Node(id='f5ff3b2b857843e4ad149a5f0f206b54', name='VectorStoreRetriever', data=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A72FD8EF60>, search_kwargs={}), metadata=None),\n",
       " '581262bf8f4842a7ae8ce1b439e7dc93': Node(id='581262bf8f4842a7ae8ce1b439e7dc93', name='Passthrough', data=RunnablePassthrough(), metadata=None),\n",
       " '56724b3603f7423391f2b5f9086156c3': Node(id='56724b3603f7423391f2b5f9086156c3', name='ChatPromptTemplate', data=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}  \\n\\nQuestion: {question}'), additional_kwargs={})]), metadata=None),\n",
       " 'e5bae8ac0e444d59a120e5821b8d38a9': Node(id='e5bae8ac0e444d59a120e5821b8d38a9', name='ChatOpenAI', data=ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001A72E6F93A0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A77FA909B0>, root_client=<openai.OpenAI object at 0x000001A72FD8E3F0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A72E6F8620>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), metadata=None),\n",
       " '7a75938d55e44d5ca93e65cfb0a9aea5': Node(id='7a75938d55e44d5ca93e65cfb0a9aea5', name='StrOutputParser', data=StrOutputParser(), metadata=None),\n",
       " '6a265a4b4c944dcd9494b53a166c159b': Node(id='6a265a4b4c944dcd9494b53a166c159b', name='StrOutputParserOutput', data=<class 'langchain_core.output_parsers.string.StrOutputParserOutput'>, metadata=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.get_graph().nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affad869",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 체인의 엣지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "190e2e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Edge(source='8e9b77ddf6dc4a9580343689d0ab8987', target='4866eacc67a640f7967bfef934ba1e15', data=None, conditional=False),\n",
       " Edge(source='4866eacc67a640f7967bfef934ba1e15', target='79c9f173efbf461aae6e72ef15086646', data=None, conditional=False),\n",
       " Edge(source='8e9b77ddf6dc4a9580343689d0ab8987', target='47a8487c9f4f423b95ce8f2f7a803671', data=None, conditional=False),\n",
       " Edge(source='47a8487c9f4f423b95ce8f2f7a803671', target='79c9f173efbf461aae6e72ef15086646', data=None, conditional=False),\n",
       " Edge(source='79c9f173efbf461aae6e72ef15086646', target='41751dca96424882967db9209ef1de0f', data=None, conditional=False),\n",
       " Edge(source='41751dca96424882967db9209ef1de0f', target='b97534ec2f4a4bca9a361695b64471c1', data=None, conditional=False),\n",
       " Edge(source='527be10f45eb4a5d86b4d91f109c3ec7', target='8a31207773eb45978fb20ae1f5909932', data=None, conditional=False),\n",
       " Edge(source='b97534ec2f4a4bca9a361695b64471c1', target='527be10f45eb4a5d86b4d91f109c3ec7', data=None, conditional=False)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.get_graph().edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca8f6a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Runnable 그래프 구조 검토\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb22825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           +---------------------------------+         \n",
      "           | Parallel<context,question>Input |         \n",
      "           +---------------------------------+         \n",
      "                    **               **                \n",
      "                 ***                   ***             \n",
      "               **                         **           \n",
      "+----------------------+              +-------------+  \n",
      "| VectorStoreRetriever |              | Passthrough |  \n",
      "+----------------------+              +-------------+  \n",
      "                    **               **                \n",
      "                      ***         ***                  \n",
      "                         **     **                     \n",
      "           +----------------------------------+        \n",
      "           | Parallel<context,question>Output |        \n",
      "           +----------------------------------+        \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                  +--------------------+               \n",
      "                  | ChatPromptTemplate |               \n",
      "                  +--------------------+               \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                      +------------+                   \n",
      "                      | ChatOpenAI |                   \n",
      "                      +------------+                   \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                   +-----------------+                 \n",
      "                   | StrOutputParser |                 \n",
      "                   +-----------------+                 \n",
      "                             *                         \n",
      "                             *                         \n",
      "                             *                         \n",
      "                +-----------------------+              \n",
      "                | StrOutputParserOutput |              \n",
      "                +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6702e6c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 프롬프트 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7acdcdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}  \\n\\nQuestion: {question}'), additional_kwargs={})])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c06f94",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## `RunnableLambda`\n",
    "\n",
    "<br>\n",
    "\n",
    "### `RunnableLambda`\n",
    "- **사용자 정의 함수를 실행할 수 있는 기능**\n",
    "  - 예) 데이터 전처리, 계산, 외부 API와의 상호 작용\n",
    "\n",
    "<br>\n",
    "\n",
    "### 사용자 정의 함수를 실행\n",
    "- **사용자 정의함수가 받을 수 있는 인자는 1개 뿐**\n",
    "\n",
    "    **만약 여러 인수를 받는 함수로 구현하고 싶다면, 단일 입력을 받아들이고 이를 여러 인수로 풀어내야 함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbeaf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a13410",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 텍스트의 길이를 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a82b52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_function(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d6b6f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 두 텍스트의 길이를 곱하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d4da0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _multiple_length_function(text1, text2):\n",
    "    return len(text1) * len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e37f7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 2개 인자를 받는 함수로 연결하는 wrapper 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfdbc2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_length_function(_dict):  \n",
    "    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5754454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}?\")\n",
    "# ChatOpenAI 모델 초기화\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3bf2df",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d761487",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"input_1\") | RunnableLambda(length_function),\n",
    "        \"b\": {\"text1\": itemgetter(\"input_1\"), \"text2\": itemgetter(\"input_2\")}\n",
    "        | RunnableLambda(multiple_length_function),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0886c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 + 9 is equal to 12.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input_1\": \"bar\", \"input_2\": \"gah\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282d624",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `RunnableConfig` 인자로 활용\n",
    "- `RunnableLambda`는 선택적으로 `Runnableconfig`를 수용할 수 있으며\n",
    "    \n",
    "    **콜백, 태그 및 기타 구성 정보를 중첩된 실행에 전달할 수 있음**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "892c6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad008842",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `\"{foo:: bar}\"`라는 잘못된 JSON을 정상적인 구조로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b590bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_or_fix(text: str, config: RunnableConfig):\n",
    "    # 텍스트를 수정하는 프롬프트 템플릿\n",
    "    fixing_chain = (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Fix the following text:\\n\\ntext\\n{input}\\n\\nError: {error}\"\n",
    "            \" Don't narrate, just respond with the fixed data.\"\n",
    "        )\n",
    "        | ChatOpenAI()\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # 파싱 중 오류가 발생하면 수정 체인을 호출하여 텍스트를 수정\n",
    "            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)\n",
    "            print(f\"config: {config}\")\n",
    "            \n",
    "    return \"Failed to parse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d41a3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff80db70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'tags': ['my-tag'], 'metadata': {}, 'callbacks': <langchain_core.callbacks.manager.CallbackManager object at 0x000001A77379BA70>, 'recursion_limit': 25, 'configurable': {}}\n",
      "\n",
      "\n",
      "수정한결과:\n",
      "{'foo': 'bar'}\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    output = RunnableLambda(parse_or_fix).invoke(\n",
    "        input=\"{foo:: bar}\",\n",
    "        config={\"tags\": [\"my-tag\"], \"callbacks\": [cb]},\n",
    "    )\n",
    "\n",
    "    print(f\"\\n\\n수정한결과:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92616ae1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## LLM 체인 라우팅 (`RunnableLambda`, `RunnableBranch`)\n",
    "\n",
    "<br>\n",
    "\n",
    "### `RunnableBranch`\n",
    "- 입력에 따라 동적으로 로직을 라우팅할 수 있는 도구\n",
    "  \n",
    "  $\\rightarrow$ 입력 데이터의 특성에 기반하여 다양한 처리 경로를 유연하게 정의\n",
    "\n",
    "- 복잡한 의사 결정 트리를 간단하고 직관적인 방식으로 구현할 수 있도록 도와주며, 이는 코드의 가독성과 유지보수성을 크게 향상\n",
    "- 또한 런타임에 동적으로 분기 조건을 평가하고 적절한 처리 루틴을 선택할 수 있어, 시스템의 적응력과 확장성을 높여줌\n",
    "- 입력 데이터의 다양성과 변동성이 큰 애플리케이션 개발에 매우 유용\n",
    "\n",
    "<br>\n",
    "\n",
    "### 입력에 따른 동적 로직 라우팅\n",
    "- 라우팅을 통해 이전 단계의 출력이 다음 단계를 정의하는 비결정적 체인을 생성할 수 있음\n",
    "- 라우팅은 LLM과의 상호 작용에 구조와 일관성을 제공하는 데 도움\n",
    "\n",
    "<br>\n",
    "\n",
    "### 라우팅을 수행하는 방법\n",
    "1. `RunnableLambda` 에서 조건부로 실행한 객체를 반환 (권장)\n",
    "2. `RunnableBranch`\n",
    "\n",
    "<br>\n",
    "\n",
    "### 예시\n",
    "- 들어오는 질문이 '수학', '과학', 또는 '기타' 중 하나로 분류하는 Chain 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54b5d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2fb6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"주어진 사용자 질문을 `수학`, `과학`, 또는 `기타` 중 하나로 분류하세요. 한 단어 이상으로 응답하지 마세요.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "602d7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "983e8b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'수학'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"2+2 는 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f548749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'과학'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"작용 반작용의 법칙은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a81a6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'기타'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Google은 어떤 회사인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230a1b7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 3개의 하위 체인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f1629b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in math. \\\n",
    "        Always answer questions starting with \"깨봉선생님께서 말씀하시기를..\". \\\n",
    "        Respond to the following question:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ae7bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "science_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in science. \\\n",
    "        Always answer questions starting with \"아이작 뉴턴 선생님께서 말씀하시기를..\". \\\n",
    "        Respond to the following question:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecb92b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question concisely:\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6750149c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 사용자 정의 함수 사용\n",
    "- **LangChain에서 권장하는 방식이며, 서로 다른 출력 간의 라우팅을 위해 사용자 정의 함수를 `RunnableLambda` 로 래핑**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d1055cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17146add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"수학\" in info[\"topic\"].lower():\n",
    "        return math_chain\n",
    "    \n",
    "    elif \"과학\" in info[\"topic\"].lower():\n",
    "        return science_chain\n",
    "    \n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe552246",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")}\n",
    "    | RunnableLambda(\n",
    "        route\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9e0360c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'깨봉선생님께서 말씀하시기를, 미적분은 수학의 한 분야로, 변화하는 양을 이해하고 분석하는 데 초점을 맞추고 있습니다. 미적분은 두 가지 주요 개념인 미분과 적분으로 나뉘어져 있습니다. \\n\\n미분은 함수의 변화율을 측정하는 과정으로, 그래프에서의 접선의 기울기를 구하는 방법이라고 할 수 있습니다. 즉, 어떤 함수가 주어졌을 때, 특정 점에서의 기울기를 파악하여 함수의 증가 또는 감소를 알 수 있게 해줍니다. 이는 물체의 속도나 가속도와 같은 변화하는 현상을 분석하는 데 매우 유용합니다.\\n\\n적분은 주어진 함수 아래의 면적을 계산하는 과정으로, 미분의 역과정이라고 볼 수 있습니다. 이는 일정 구간에서의 총량을 구하거나, 물리적인 의미에서 총합을 계산하는 데 사용됩니다. 예를 들어, 물체가 이동한 거리나 평균값을 구할 때 적분을 활용합니다.\\n\\n미적분은 이러한 미분과 적분의 원리를 통해 실제 문제를 해결하고, 자연현상을 수학적으로 모델링하는 기본 도구로 자리잡고 있습니다.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"미적분의 개념에 대해 말씀해 주세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d50a7192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아이작 뉴턴 선생님께서 말씀하시기를, 중력은 두 질량 간의 상호작용으로 발생하는 힘이며, 그 크기는 두 질량의 곱에 비례하고, 그들 사이의 거리의 제곱에 반비례한다는 것입니다. 이는 만유인력의 법칙으로 표현되며, 모든 물체는 서로에게 중력을 끌어당기는 힘을 발휘합니다. 이 힘은 지구와 같은 큰 천체가 물체를 끌어당기며, 우리가 지구 표면에 머물 수 있도록 하는 근본적인 원리입니다.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"중력은 어떻게 작용하나요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73560d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG(Retrieval Augmented Generation)은 정보 검색(retrieval)과 생성(generation) 기법을 결합한 자연어 처리 모델입니다. 이 모델은 주어진 질문에 대한 답변을 생성하기 위해 외부 데이터베이스에서 관련 정보를 검색하고, 이 정보를 사용하여 더 정확하고 풍부한 응답을 생성합니다. RAG는 검색된 정보를 이용해 문맥을 제공함으로써, 보다 유용하고 정보가 풍부한 결과를 도출할 수 있도록 돕습니다.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"RAG(Retrieval Augmented Generation)은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de6540",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `RunnableBranch`\n",
    "- `RunnableBranch`는 입력값에 따라 실행할 조건과 `Runnable`을 정의할 수 있는 특별한 유형의 `Runnable`\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 문법\n",
    "- `RunnableBranch는` (조건, Runnable) 쌍의 리스트와 기본 `Runnable`로 초기화\n",
    "- 호출 시 전달된 입력값을 각 조건에 전달하여 분기를 선택\n",
    "- `True`로 평가되는 첫 번째 조건을 선택하고, 해당 조건에 해당하는 `Runnable`을 입력값과 함께 실행\n",
    "- 제공된 조건과 일치하는 것이 없으면 기본 `Runnable` 을 실행\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d561241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf7777c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "branch = RunnableBranch(\n",
    "    # 주제에 \"수학\"이 포함되어 있는지 확인하고, 포함되어 있다면 math_chain을 실행\n",
    "    (lambda x: \"수학\" in x[\"topic\"].lower(), math_chain),\n",
    "    # 주제에 \"과학\"이 포함되어 있는지 확인하고, 포함되어 있다면 science_chain을 실행\n",
    "    (lambda x: \"과학\" in x[\"topic\"].lower(), science_chain),\n",
    "    # 위의 조건에 해당하지 않는 경우 general_chain을 실행\n",
    "    general_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c508e8",
   "metadata": {},
   "source": [
    "- 주제와 질문을 입력받아 branch를 실행하는 전체 체인을 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e446e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")} | branch | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bfd6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'깨봉선생님께서 말씀하시기를 미적분은 수학의 두 중요한 영역인 미분과 적분을 포함하는 개념으로, 변화와 누적의 원리를 연구하는 학문입니다. 미분은 함수의 변화율을 측정하며, 특정 구간에서의 함수의 기울기, 즉 순간적인 변화량을 구하는 데 사용됩니다. 반면, 적분은 특정 구간에서 함수의 전체적인 값을 누적하고, 넓이나 부피를 계산하는 데 활용됩니다. 미적분의 기본정리는 이 두 가지 개념이 서로 밀접하게 연결되어 있음을 보여주며, 많은 과학 및 공학 분야에서 필수적인 도구로 사용됩니다.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"미적분의 개념에 대해 말씀해 주세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41f76251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아이작 뉴턴 선생님께서 말씀하시기를, 중력 가속도는 물체가 지구의 중력에 의해 끌려내려가는 가속도를 나타내며, 일반적으로 약 9.81 m/s²로 알려져 있습니다. 중력 가속도를 계산하기 위해서는 다음과 같은 일반적인 방법을 사용할 수 있습니다.\\n\\n1. 물체의 질량(m)과 중력의 영향을 받는 높이(h)를 측정합니다.\\n2. 중력 가속도(g)는 다음의 중력 법칙을 사용하여 계산할 수 있습니다: \\n   \\\\[\\n   g = \\\\frac{F}{m}\\n   \\\\]\\n   여기서 F는 중력의 힘으로, 이는 \\\\( F = m \\\\cdot g \\\\)라는 식을 다시 변형한 것입니다.\\n3. 지구의 중력장은 평균적으로 약 9.81 m/s²로 일정하므로, 이 값을 직접 사용할 수도 있습니다.\\n\\n이러한 계산을 통해 중력 가속도를 확인할 수 있으며, 실험적 방법을 통해도 이를 측정할 수 있습니다.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"중력 가속도는 어떻게 계산하나요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7eda41d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG(Retrieval Augmented Generation)는 정보 검색과 자연어 생성 기술을 결합한 모델로, 주어진 질문에 대해 관련 정보를 외부 데이터베이스에서 검색하여 해당 정보를 토대로 응답을 생성하는 방식입니다. 이를 통해 보다 정확하고 풍부한 답변을 제공할 수 있습니다.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"RAG(Retrieval Augmented Generation)은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216ff31",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## `RunnableParallel`\n",
    "\n",
    "<br>\n",
    "\n",
    "### 입력 및 출력 조작\n",
    "- `RunnableParallel` 은 시퀀스 내에서 하나의 `Runnable` 의 출력을 다음 `Runnable` 의 입력 형식에 맞게 조작하는 데 유용하게 사용\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2f8e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "572bbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Teddy is an AI engineer who loves programming!\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8edd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f8d1bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Teddy's occupation is an AI engineer.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"What is Teddy's occupation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb8e57",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `Runnable`과 함께 `RunnableParallel`을 구성할 때, 유형 변환이 자동으로 처리되므로, `RunnableParallel`클래스에서 입력으로 주입되는 `dict`입력을 별도 래핑할 필요가 없음\n",
    "\n",
    "<br>\n",
    "\n",
    "- 아래의 3가지 방식은 모두 동일하게 처리\n",
    "\n",
    "```\n",
    "- 자체 RunnableParallel 로 래핑됨\n",
    "1. {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "\n",
    "2. RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "\n",
    "3. RunnableParallel(context=retriever, question=RunnablePassthrough())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f96cd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `itemgetter`를 단축어로 사용\n",
    "- `RunnableParallel`과 결합할 때 Python의 `itemgetter`를 단축어로 사용하여 `map`에서 데이터를 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65174676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "beeaee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Teddy is an AI engineer who loves programming!\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cf5126d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테디의 직업은 AI 엔지니어입니다.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is Teddy's occupation?\", \"language\": \"Korean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6339a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 병렬처리를 단계별로 이해\n",
    "- `RunnableParallel`을 사용하면 여러 `Runnable`을 병렬로 실행하고, 이러한 `Runnable`의 출력을 맵(`map`)으로 반환하는 것이 용이해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1038064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9e9948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': '대한민국의 수도는 서울 입니다.', 'area': '대한민국의 면적은 약 100,363제곱킬로미터 입니다.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI() \n",
    "\n",
    "capital_chain = (\n",
    "    ChatPromptTemplate.from_template(\"{country} 의 수도는 어디입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "area_chain = (\n",
    "    ChatPromptTemplate.from_template(\"{country} 의 면적은 얼마입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(capital=capital_chain, area=area_chain)\n",
    "\n",
    "map_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fce874",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `chain`별로 입력 템플릿 변수가 달라도 상관없이 실행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63282732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': '대한민국의 수도는 서울입니다.', 'area': '미국의 면적은 약 9.8백만 제곱 킬로미터입니다.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital_chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"{country1} 의 수도는 어디입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "area_chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"{country2} 의 면적은 얼마입니까?\")\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "map_chain2 = RunnableParallel(capital=capital_chain2, area=area_chain2)\n",
    "\n",
    "map_chain2.invoke({\"country1\": \"대한민국\", \"country2\": \"미국\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f13655",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 병렬 처리\n",
    "- `RunnableParallel` 은 맵에 있는 각 `Runnable` 이 병렬로 실행되기 때문에 독립적인 프로세스를 병렬로 실행하는 데에도 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ead44eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649 ms ± 145 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "area_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759784c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 ms ± 369 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "capital_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "332086e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836 ms ± 271 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"country\": \"대한민국\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0a6c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
