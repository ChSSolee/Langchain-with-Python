{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7604914",
   "metadata": {},
   "source": [
    "# 4. 모델(Model)\n",
    "- 모델 혹은 LLM(Large Language Model) 단계는 이전 프롬프트 단계에서 구성된 입력을 기반으로 대규모 언어 모델을 활용하여 응답을 생성하는 과정\n",
    "- 이 단계는 RAG 시스템의 핵심적인 부분으로, 언어 모델의 능력을 최대한 활용하여 사용자의 질문에 대해 정확하고 자연스러운 답변을 생성\n",
    "  \n",
    "\n",
    "<br>\n",
    "\n",
    "### LLM의 필요성\n",
    "- **사용자 의도 이해**: LLM은 다양한 언어의 구조와 의미를 깊이 이해하고 있으며, 이를 바탕으로 복잡한 질문에 답할 수 있음\n",
    "  - 자연어 이해(NLU)와 자연어 생성(NLG) 능력이 결합되어, 보다 자연스럽고 유익한 응답을 제공\n",
    "\n",
    "- **문맥적 적응성** : LLM은 주어진 문맥을 고려하여 응답을 생성\n",
    "  - 이는 사용자의 질문에 더욱 정확하게 대응할 수 있으며, 사전학습된 지식외 사용자가 제공한 정보에 기반한 답변을 문맥을 참고하여 답변\n",
    "\n",
    "<br>\n",
    "\n",
    "### LLM의 중요성\n",
    "- LLM 단계는 사용자의 질문에 대한 답변의 질과 자연스러움을 결정짓는 핵심 요소\n",
    "  \n",
    "  LLM은 지금까지의 모든 데이터와 정보를 종합하여 사용자의 질문에 최적화된 답변을 생성 \n",
    "- LLM의 성능은 RAG 시스템의 전체적인 성능과 사용자 만족도에 직접적으로 영향을 미치며, 이는 RAG 시스템을 사용하는 많은 응용 분야에서 매우 중요한 역할\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 04-01. 다양한 LLM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed194b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f5bd4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### OpenAI\n",
    "- OpenAI는 채팅 전용 Large Language Model (LLM)을 제공\n",
    "- 이 모델을 생성할 때 다양한 옵션을 지정할 수 있으며, 이러한 옵션들은 모델의 동작 방식에 영향\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 옵션 상세 설명\n",
    "- `temperature` : 샘플링 온도를 설정하는 옵션\n",
    "  - 0과 2 사이에서 선택 : 높은 값(예: 0.8)은 출력을 더 무작위하게 만들고, 낮은 값(예: 0.2)은 출력을 더 집중되고 결정론적으로\n",
    "\n",
    "- `max_tokens` : 채팅 완성에서 생성할 토큰의 최대 개수를 지정\n",
    "  - 이 옵션은 모델이 한 번에 생성할 수 있는 텍스트의 길이를 제어\n",
    "- `model_name` : [적용 가능한 모델을 선택하는 옵션](https://platform.openai.com/docs/models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334dab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58094db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사랑은 매우 복잡하고 다차원적인 감정으로, 사람마다 다르게 정의될 수 있습니다. 일반적으로 사랑은 깊은 애정과 헌신, 그리고 타인에 대한 배려와 이해를 포함하는 감정입니다. 사랑은 가족, 친구, 연인 등 다양한 관계에서 나타날 수 있으며, 각 관계마다 그 형태와 표현 방식이 다를 수 있습니다.\n",
      "\n",
      "사랑은 또한 사람에게 큰 행복과 만족을 줄 수 있지만, 때로는 고통과 어려움을 동반하기도 합니다. 사랑은 인간의 삶에서 중요한 부분을 차지하며, 문학, 예술, 음악 등 다양한 분야에서 영감의 원천이 되기도 합니다."
     ]
    }
   ],
   "source": [
    "gpt = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-4o\"\n",
    ")\n",
    "answer = gpt.stream(\"사랑이 뭔가요?\")\n",
    "for chunk in answer:\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8686db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Anthropic\n",
    "- Anthropic은 인공지능(AI) 안전성과 연구에 중점을 둔 미국의 스타트업\n",
    "\n",
    "  - 설립 연도: 2021년\n",
    "  - 위치: 미국 샌프란시스코\n",
    "  - 창립자: OpenAI 출신 직원들 (Daniela Amodei와 Dario Amodei 등)\n",
    "  - 기업 형태: 공익기업(Public Benefit Corporation)으로 등록\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Claude\n",
    "- Claude는 Anthropic의 대표적인 대규모 언어 모델(LLM) 제품군\n",
    "    - API 키 발급: https://console.anthropic.com/settings/keys\n",
    "    - 모델 리스트: https://docs.anthropic.com/en/docs/about-claude/models\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "anthropic = ChatAnthropic(model_name=\"claude-3-5-sonnet-20241022\")\n",
    "stream = anthropic.stream([HumanMessage(content=\"사랑이 뭔가요?\")])\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.content or chunk.delta, end='', flush=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423be11b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Perplexity\n",
    "- https://www.perplexity.ai/\n",
    "\n",
    "<br>\n",
    "\n",
    "* API 키 발급 후 `.env` 파일에 키 저장\n",
    "\n",
    "```env\n",
    "PPLX_API_KEY=이곳에 API 키를 입력하세요.\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 매개변수\n",
    "- `model` : 사용할 언어 모델을 지정 (예: \"llama-3.1-sonar-small-128k-online\") - 기본 성능과 능력을 결정.\n",
    "\n",
    "- `temperature` : 응답의 무작위성을 조절 (0.0-1.0), 0은 결정적, 1은 가장 무작위한 응답 생성.\n",
    "\n",
    "- `top_p` : 토큰 샘플링의 확률 임계값 설정 (0.0-1.0), 높을수록 더 다양한 출력 허용.\n",
    "\n",
    "- `search_domain_filter` : 검색 결과를 지정된 도메인으로 제한, 리스트 형태로 제공 (예: [\"perplexity.ai\"]).\n",
    "\n",
    "- `return_images` : 응답에 이미지 포함 여부를 결정하는 불리언 플래그.\n",
    "\n",
    "- `return_related_questions` : 관련 질문 제안 기능을 활성화/비활성화하는 불리언 플래그.\n",
    "\n",
    "- `top_k` : 사용할 검색 결과의 수 제한 (0은 제한 없음을 의미).\n",
    "\n",
    "- `streaming` : 응답을 스트리밍으로 받을지 완성된 형태로 받을지 결정하는 불리언 플래그.\n",
    "\n",
    "- `presence_penalty` : 토큰 반복에 대한 페널티 (-2.0에서 2.0), 높을수록 재사용을 억제.\n",
    "\n",
    "- `frequency_penalty` : 일반적/희귀 토큰 선호도 조정 (-2.0에서 2.0), 높을수록 희귀 토큰 선호."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43743025",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "```python\n",
    "from langchain_teddynote.models import ChatPerplexity\n",
    "\n",
    "perplexity = ChatPerplexity(\n",
    "    model=\"llama-3.1-sonar-large-128k-online\",\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    search_domain_filter=[\"perplexity.ai\"],\n",
    "    return_images=False,\n",
    "    return_related_questions=True,\n",
    "    # search_recency_filter=\"month\",\n",
    "    top_k=0,\n",
    "    streaming=False,\n",
    "    presence_penalty=0,\n",
    "    frequency_penalty=1,\n",
    ")\n",
    "\n",
    "response = perplexity.invoke(\"2024년 노벨문학상 수상자를 조사해 주세요\")\n",
    "print(response.content)\n",
    "\n",
    "print()\n",
    "for i, citation in enumerate(response.citations):\n",
    "    print(f\"[{i+1}] {citation}\")\n",
    "\n",
    "# 스트리밍 출력\n",
    "response = perplexity.stream(\"2024년 노벨문학상 수상자를 조사해 주세요\")\n",
    "\n",
    "for token in response:\n",
    "    print(token.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "for i, citation in enumerate(token.citations):\n",
    "    print(f\"[{i+1}] {citation}\")\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 04-02. 캐싱(Cache)\n",
    "- LangChain은 LLM을 위한 선택적 캐싱 레이어를 제공\n",
    "    - 동일한 완료를 여러 번 요청하는 경우 LLM 공급자에 대한 API 호출 횟수를 줄여 비용을 절감\n",
    "    - LLM 제공업체에 대한 API 호출 횟수를 줄여 애플리케이션의 속도를 높일 수 있음\n",
    "\n",
    "<br>\n",
    "\n",
    "* 모델과 프롬프트를 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44740d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "213631d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 고도 경제성장을 이룬 선진국가로, 서울을 수도로 하는 대한민국과 평양을 수도로 하는 북한으로 이루어져 있다. 한반도에 위치하며 미세먼지 등 환경문제, 북한과의 관계, 남북간의 경제 협력 등 다양한 이슈를 안고 있다. 전통적인 문화와 현대화된 도시 모습을 동시에 보여주며 K-pop, K-drama 등 문화 콘텐츠로 세계적인 인기를 누리고 있다. 또한 IT 산업과 자동차 산업 등 다양한 산업 분야에서 선두를 달리고 있으며, 한류 열풍으로 인해 관광 산업도 크게 성장하고 있다.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "prompt = PromptTemplate.from_template(\"{country} 에 대해서 200자 내외로 요약해줘\")\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad99165",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `InMemoryCache`\n",
    "- 인메모리 캐시를 사용하여 동일 질문에 대한 답변을 저장하고, **캐시에 저장된 답변을 반환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a764b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cf50e",
   "metadata": {},
   "source": [
    "- 인메모리 캐시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3139ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396169f",
   "metadata": {},
   "source": [
    "- 1.9초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e572921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한반도의 동쪽에 위치한 대한민국은 수도인 서울을 중심으로 현대화된 도시와 전통적인 문화가 공존하는 나라이다. 미세먼지와 같은 환경문제와 경제 성장, 사회 불평등 등 다양한 과제를 안고 있는 동시에 K-pop, K-drama 같은 대중문화 콘텐츠로 국제적으로 큰 인기를 얻고 있다. 또한 한류와 한식, 한국어 등을 통해 한국문화에 대한 글로벌 관심이 높아지고 있다. 정치적으로는 북한과의 관계 개선을 모색하고 있으며, 경제적으로는 기술혁신과 산업 발전을 통해 세계적인 경쟁력을 확보하고 있다. 현재는 코로나19 대응과 함께 미래를 준비하는 중요한 시기에 있다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c2022",
   "metadata": {},
   "source": [
    "- 동일한 질문 다시 $\\rightarrow$ 0.0초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d99095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한반도의 동쪽에 위치한 대한민국은 수도인 서울을 중심으로 현대화된 도시와 전통적인 문화가 공존하는 나라이다. 미세먼지와 같은 환경문제와 경제 성장, 사회 불평등 등 다양한 과제를 안고 있는 동시에 K-pop, K-drama 같은 대중문화 콘텐츠로 국제적으로 큰 인기를 얻고 있다. 또한 한류와 한식, 한국어 등을 통해 한국문화에 대한 글로벌 관심이 높아지고 있다. 정치적으로는 북한과의 관계 개선을 모색하고 있으며, 경제적으로는 기술혁신과 산업 발전을 통해 세계적인 경쟁력을 확보하고 있다. 현재는 코로나19 대응과 함께 미래를 준비하는 중요한 시기에 있다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c9d96",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### SQLite Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c365e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_core.globals import set_llm_cache\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c25a2f",
   "metadata": {},
   "source": [
    "- 캐시 디렉토리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44ca8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"cache\"):\n",
    "    os.makedirs(\"cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0b18",
   "metadata": {},
   "source": [
    "- SQLiteCache를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f59591",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_llm_cache(SQLiteCache(database_path=\"cache/llm_cache.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b453f",
   "metadata": {},
   "source": [
    "- 1.9초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d5243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 대한민국과 조선민주주의인민공화국으로 나뉜 국가이다. 대한민국은 민주주의를 기반으로 한 고도의 경제 성장을 이룩한 선진국가이며 IT 기술과 자동차 산업 등이 세계적으로 유명하다. 또한 한류 열풍으로 한국 문화도 세계적으로 인기를 끌고 있다. 반면 조선민주주의인민공화국은 공산주의 체제를 유지하고 있으며 국제 사회와의 교류가 제한되어 있으며 인권 문제와 미사일 개발로 논란이 지속되고 있다. 함께 한반도를 이루는 두 국가는 과거의 역사와 정치적인 이슈로 계속해서 관심을 받고 있다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036ceff",
   "metadata": {},
   "source": [
    "- 동일한 질문 다시 $\\rightarrow$ 0.0초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f612d259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국은 동아시아에 위치한 대한민국과 조선민주주의인민공화국으로 나뉜 국가이다. 대한민국은 민주주의를 기반으로 한 고도의 경제 성장을 이룩한 선진국가이며 IT 기술과 자동차 산업 등이 세계적으로 유명하다. 또한 한류 열풍으로 한국 문화도 세계적으로 인기를 끌고 있다. 반면 조선민주주의인민공화국은 공산주의 체제를 유지하고 있으며 국제 사회와의 교류가 제한되어 있으며 인권 문제와 미사일 개발로 논란이 지속되고 있다. 함께 한반도를 이루는 두 국가는 과거의 역사와 정치적인 이슈로 계속해서 관심을 받고 있다.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"country\": \"한국\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf17f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 모델 직렬화(Serialization) - 저장 및 불러오기\n",
    "\n",
    "<br>\n",
    "\n",
    "### 직렬화(Serialization)\n",
    "- **모델을 저장 가능한 형식으로 변환하는 과정**\n",
    "  - 모델 재사용 (재훈련 없이)\n",
    "  - 모델 배포 및 공유 용이\n",
    "  - 계산 리소스 절약\n",
    "  - 빠른 모델 로딩\n",
    "  - 버전 관리 가능\n",
    "  - 다양한 환경에서 사용 가능\n",
    "- 모델 직렬화는 AI 개발 및 배포 과정에서 중요한 단계로, 효율적인 모델 관리와 재사용을 가능하게 함\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `is_lc_serializable` 클래스 메서드로 실행하여 LangChain 클래스가 직렬화 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44cebcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4902c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{fruit}의 색상이 무엇입니까?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122b3b9",
   "metadata": {},
   "source": [
    "- llm 객체에 대하여 직렬화 가능 여부를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d16d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatOpenAI: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"ChatOpenAI: {ChatOpenAI.is_lc_serializable()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d9f0a",
   "metadata": {},
   "source": [
    "- chain 객체에 대하여 직렬화 가능 여부를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cbbd52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain.is_lc_serializable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fffb25",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### 체인(Chain) 직렬화(dumps, dumpd)\n",
    "- **체인 직렬화는 직렬화 가능한 모든 객체를 딕셔너리 또는 JSON 문자열로 변환하는 과정을 의미**\n",
    "\n",
    "<br>\n",
    "\n",
    "- **직렬화 방법**\n",
    "  - 객체의 속성 및 데이터를 키-값 쌍으로 저장하여 딕셔너리 형태로 변환\n",
    "  - 이러한 직렬화 방식은 객체를 쉽게 저장하고 전송할 수 있게 하며, 다양한 환경에서 객체를 재구성할 수 있도록 함\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- `dumpd`: 객체를 딕셔너리로 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "548630d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'runnable', 'RunnableSequence'],\n",
       " 'kwargs': {'first': {'lc': 1,\n",
       "   'type': 'constructor',\n",
       "   'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],\n",
       "   'kwargs': {'input_variables': ['fruit'],\n",
       "    'template': '{fruit}의 색상이 무엇입니까?',\n",
       "    'template_format': 'f-string'},\n",
       "   'name': 'PromptTemplate'},\n",
       "  'last': {'lc': 1,\n",
       "   'type': 'constructor',\n",
       "   'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       "   'kwargs': {'model_name': 'gpt-3.5-turbo',\n",
       "    'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']},\n",
       "    'output_version': 'v0'},\n",
       "   'name': 'ChatOpenAI'}},\n",
       " 'name': 'RunnableSequence'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.load import dumpd, dumps\n",
    "\n",
    "dumpd_chain = dumpd(chain)\n",
    "dumpd_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e0dc77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dumpd_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d368d4",
   "metadata": {},
   "source": [
    "- `dumps`: 객체를 JSON 문자열로 직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007a85d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableSequence\"], \"kwargs\": {\"first\": {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"prompts\", \"prompt\", \"PromptTemplate\"], \"kwargs\": {\"input_variables\": [\"fruit\"], \"template\": \"{fruit}\\\\uc758 \\\\uc0c9\\\\uc0c1\\\\uc774 \\\\ubb34\\\\uc5c7\\\\uc785\\\\ub2c8\\\\uae4c?\", \"template_format\": \"f-string\"}, \"name\": \"PromptTemplate\"}, \"last\": {\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"model_name\": \"gpt-3.5-turbo\", \"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}, \"output_version\": \"v0\"}, \"name\": \"ChatOpenAI\"}}, \"name\": \"RunnableSequence\"}'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumps_chain = dumps(chain)\n",
    "dumps_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad759581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dumps_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4195d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### `Pickle` 파일\n",
    "- Pickle 파일은 Python 객체를 바이너리 형태로 직렬화하는 포맷\n",
    "  - Python 전용 (다른 언어와 호환 불가)\n",
    "  - 대부분의 Python 데이터 타입 지원 (리스트, 딕셔너리, 클래스 등)\n",
    "  - 객체의 상태와 구조를 그대로 보존\n",
    "\n",
    "- 보안 위험 (신뢰할 수 없는 데이터 역직렬화 시 주의 필요)\n",
    "- 사람이 읽을 수 없는 바이너리 형식\n",
    "- 용도\n",
    "  - 객체 캐싱\n",
    "  - 머신러닝 모델 저장\n",
    "  - 프로그램 상태 저장 및 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468dd5d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `pickle.dump()`: 객체를 파일에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ccabadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./data/fruit_chain.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dumpd_chain, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bff06718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/fruit_chain.json\", \"w\") as fp:\n",
    "    json.dump(dumpd_chain, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53a5e2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `pickle.load()`: 파일에서 객체 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d321f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2384862e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='사과의 색상은 주로 빨간색이지만, 녹색, 황색, 주황색 등 다양한 색상의 사과도 존재합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 24, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CM80Q4kmkZr7DcHqWYT1sqG3aYM2S', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dc1140f8-97af-4a2d-bb59-9e5c955ed133-0', usage_metadata={'input_tokens': 24, 'output_tokens': 50, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}, 'total_cost': 0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/fruit_chain.pkl\", \"rb\") as f:\n",
    "    loaded_chain = pickle.load(f)\n",
    "    \n",
    "load_chain = load(\n",
    "    loaded_chain, secrets_map={\"OPENAI_API_KEY\": os.environ[\"OPENAI_API_KEY\"]}\n",
    ")\n",
    "load_chain.invoke({\"fruit\": \"사과\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bff0bcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='사과의 색상은 주로 빨간색이지만, 녹색, 황색, 주황색 등 다양한 색상의 사과도 존재합니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 24, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CM80Q4kmkZr7DcHqWYT1sqG3aYM2S', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dc1140f8-97af-4a2d-bb59-9e5c955ed133-0', usage_metadata={'input_tokens': 24, 'output_tokens': 50, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}, 'total_cost': 0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/fruit_chain.json\", \"r\") as fp:\n",
    "    loaded_from_json_chain = json.load(fp)\n",
    "    loads_chain = load(loaded_from_json_chain)\n",
    "\n",
    "loads_chain.invoke({\"fruit\": \"사과\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c5683",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 04-04. 토큰 사용량 확인\n",
    "- 이 기능은 현재 OpenAI API 에만 구현\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b1f6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee5fca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98207190",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### `with get_openai_callback()` 구문안에서 실행되는 모든 토큰 사용량/요금이 추적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3b240cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 사용된 토큰수: \t\t46\n",
      "프롬프트에 사용된 토큰수: \t30\n",
      "답변에 사용된 토큰수: \t16\n",
      "호출에 청구된 금액(USD): \t$0.00023500000000000002\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    result = llm.invoke(\"대한민국의 수도는 어디야?\")\n",
    "    print(f\"총 사용된 토큰수: \\t\\t{cb.total_tokens}\")\n",
    "    print(f\"프롬프트에 사용된 토큰수: \\t{cb.prompt_tokens}\")\n",
    "    print(f\"답변에 사용된 토큰수: \\t{cb.completion_tokens}\")\n",
    "    print(f\"호출에 청구된 금액(USD): \\t${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ff899",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 04-05. 구글 생성 AI(Google Generative AI)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Google AI chat models (gemini-pro)\n",
    "- Google AI의 `gemini`와 `gemini-vision` 모델뿐만 아니라 다른 생성 모델에 접근하려면 `langchain-google-genai` 통합 패키지의 `ChatGoogleGenerativeAI` 클래스를 사용\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "pip install -qU langchain-google-genai\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "### API KEY\n",
    "- https://aistudio.google.com/app/api-keys\n",
    "- `.env`에 환경변수 추가 `GOOGLE_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b88b6da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c737b6",
   "metadata": {},
   "source": [
    "- `langchain_google_genai` 패키지에서 `ChatGoogleGenerativeAI` 클래스를 호출\n",
    "    - `ChatGoogleGenerativeAI` 클래스는 Google의 Generative AI 모델을 사용하여 대화형 AI 시스템을 구현하는 데 사용\n",
    "    - 모델과의 대화는 채팅 형식으로 이루어지며, 사용자의 입력에 따라 모델이 적절한 응답을 생성\n",
    "    - `ChatGoogleGenerativeAI` 클래스는 `LangChain` 프레임워크와 통합되어 있어, 다른 LangChain 컴포넌트와 함께 사용 가능\n",
    "- 지원되는 모델 정보 : https://ai.google.dev/gemini-api/docs/models/gemini?hl=ko\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e5da39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d71ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 자연어 처리 (Natural Language Processing, NLP) 간략 설명\n",
      "\n",
      "자연어 처리는 **컴퓨터가 인간의 언어 (자연어)를 이해하고, 처리하고, 생성할 수 있도록 하는 기술**입니다. 쉽게 말해, 컴퓨터가 사람의 말을 알아듣고, 그 의미를 파악해서, 필요한 작업을 수행하거나 사람처럼 말할 수 있게 하는 분야입니다.\n",
      "\n",
      "**핵심 목표:**\n",
      "\n",
      "*   **이해 (Understanding):** 텍스트나 음성 언어의 의미, 맥락, 의도를 파악합니다.\n",
      "*   **생성 (Generation):** 인간의 언어와 유사한 텍스트나 음성을 만들어냅니다.\n",
      "\n",
      "**주요 기술:**\n",
      "\n",
      "*   **텍스트 분석 (Text Analysis):**\n",
      "    *   **토큰화 (Tokenization):** 문장을 단어, 구, 문장 등으로 쪼개는 과정.\n",
      "    *   **형태소 분석 (Morphological Analysis):** 단어의 형태를 분석 (어근, 접사, 어미 등).\n",
      "    *   **구문 분석 (Parsing):** 문장의 문법 구조를 분석 (주어, 동사, 목적어 등).\n",
      "    *   **의미 분석 (Semantic Analysis):** 단어와 문장의 의미를 파악.\n",
      "*   **자연어 생성 (Natural Language Generation):**\n",
      "    *   **텍스트 생성:** 주어진 정보나 데이터를 바탕으로 텍스트를 생성.\n",
      "    *   **대화 시스템 (Dialogue Systems):** 챗봇 등과 같은 대화형 시스템 개발.\n",
      "*   **기계 번역 (Machine Translation):** 한 언어를 다른 언어로 번역.\n",
      "*   **정보 검색 (Information Retrieval):** 원하는 정보를 검색.\n",
      "*   **감성 분석 (Sentiment Analysis):** 텍스트의 감정 (긍정, 부정, 중립)을 파악.\n",
      "*   **질의 응답 (Question Answering):** 질문에 대한 답을 찾아 제시.\n",
      "*   **텍스트 요약 (Text Summarization):** 긴 텍스트를 요약.\n",
      "\n",
      "**활용 분야:**\n",
      "\n",
      "*   **챗봇 및 가상 비서:** 고객 응대, 정보 제공, 예약 등\n",
      "*   **번역 서비스:** 구글 번역, 파파고 등\n",
      "*   **검색 엔진:** 검색 결과의 정확도 향상\n",
      "*   **소셜 미디어 분석:** 여론 분석, 감성 분석\n",
      "*   **스팸 메일 필터링:** 스팸 메일, 악성 댓글 등\n",
      "*   **자동 문서 작성:** 보고서, 기사 등\n",
      "*   **의료 분야:** 의료 기록 분석, 질병 진단 보조 등\n",
      "\n",
      "**최근 동향:**\n",
      "\n",
      "*   **딥러닝 (Deep Learning) 기술의 발전:** 특히, **BERT, GPT** 등과 같은 대규모 언어 모델 (LLM)의 등장으로 NLP 성능이 크게 향상되었습니다.\n",
      "*   **다양한 언어 지원 확대:** 한국어를 포함한 다양한 언어를 처리하는 기술 개발.\n",
      "*   **윤리적 문제에 대한 관심 증가:** 편향성, 개인정보 보호 등\n",
      "\n",
      "**결론:**\n",
      "\n",
      "자연어 처리는 우리 주변에서 점점 더 중요한 역할을 하고 있으며, 앞으로도 더욱 발전하여 인간과 컴퓨터 간의 소통을 더욱 자연스럽게 만들 것으로 기대됩니다."
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-001\")\n",
    "answer = llm.stream(\"자연어처리에 대해서 간략히 설명해 줘\")\n",
    "\n",
    "for chunk in answer:\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7a0fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3eeb817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예."
     ]
    }
   ],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite-001\")\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"예/아니오 질문에 대답하세요. {question}는 과일입니까?\"\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "for chunk in chain.stream({\"question\": \"사과\"}):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903faef2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### [Safety Settings](https://ai.google.dev/gemini-api/docs/safety-settings?hl=ko)\n",
    "- Gemini 모델에는 기본 안전 설정(Satety Settings) 이 있지만, 이를 재정의 가능\n",
    "    - 만약 모델로부터 많은 \"Safety Warnings\"를 받고 있다면, 모델의 `safety_settings` 속성을 조정\n",
    "- Google의 Safety Setting Types 문서에서는 사용 가능한 카테고리와 임계값에 대한 열거형 정보를 제공\n",
    "\n",
    "  - 이 문서에는 콘텐츠 필터링 및 안전 설정과 관련된 다양한 카테고리와 해당 임계값이 정의되어 있어, 개발자들이 생성형 AI 모델을 활용할 때 적절한 안전 설정을 선택하고 적용하는 데 도움\n",
    "\n",
    "    $\\rightarrow$ 모델이 생성하는 콘텐츠의 안전성과 적절성을 보장하고, 사용자에게 유해하거나 부적절한 내용이 노출되는 것을 방지\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dc6b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import (\n",
    "    ChatGoogleGenerativeAI,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97dc2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite-001\",\n",
    "    safety_settings={\n",
    "        # 위험한 콘텐츠에 대한 차단 임계값을 설정\n",
    "        # 이 경우 위험한 콘텐츠를 차단하지 않도록 설정 (그럼에도 기본적인 차단이 있을 수 있음.)\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e25a6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Batch 단위 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "197eb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite-001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc532d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국의 수도는 **서울**입니다.\n",
      "대한민국의 주요 관광지 5곳은 다음과 같습니다:\n",
      "\n",
      "1.  **경복궁 (서울)**: 조선 시대의 정궁으로, 아름다운 궁궐 건축과 정원을 감상할 수 있습니다. 특히 봄에는 벚꽃, 가을에는 단풍이 어우러져 더욱 아름다운 풍경을 자랑합니다.\n",
      "\n",
      "2.  **해운대 (부산)**: 넓은 백사장과 시원한 바다를 만끽할 수 있는 해변으로, 다양한 해양 레저 활동과 축제를 즐길 수 있습니다. 밤에는 화려한 야경도 볼 수 있습니다.\n",
      "\n",
      "3.  **불국사 & 석굴암 (경주)**: 신라 시대의 불교 문화 유적지로, 유네스코 세계문화유산으로 지정되어 있습니다. 섬세한 건축 양식과 역사적 가치를 느낄 수 있습니다.\n",
      "\n",
      "4.  **남산 & N서울타워 (서울)**: 서울의 랜드마크로, 서울 시내를 한눈에 조망할 수 있는 전망대와 다양한 볼거리가 있습니다. 케이블카를 타고 올라가는 낭만적인 경험도 할 수 있습니다.\n",
      "\n",
      "5.  **제주도 (제주)**: 아름다운 자연 경관을 자랑하는 섬으로, 한라산, 성산일출봉, 용머리 해안 등 다채로운 볼거리가 있습니다. 올레길을 따라 걷는 트레킹도 인기 있습니다.\n"
     ]
    }
   ],
   "source": [
    "results = llm.batch(\n",
    "    [\n",
    "        \"대한민국의 수도는?\",\n",
    "        \"대한민국의 주요 관광지 5곳을 나열하세요\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddc612",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Multimodal 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f635bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fac86150",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite-001\",\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"당신은 시인입니다. 당신의 임무는 주어진 이미지를 가지고 시를 작성하는 것입니다.\"\n",
    ")\n",
    "\n",
    "user_prompt = \"다음의 이미지에 대한 시를 작성해주세요.\"\n",
    "\n",
    "IMAGE_URL = \"images/jeju-beach.jpg\"\n",
    "\n",
    "\n",
    "with open(IMAGE_URL, \"rb\") as f:\n",
    "    img_base64 = base64.b64encode(f.read()).decode()\n",
    "\n",
    "image_data_uri = f\"data:image/png;base64,{img_base64}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bf6b89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물결이 부드럽게 부서지고,\n",
      "하늘이 맑고 파랗다.\n",
      "멀리 솟아오른 섬,\n",
      "이 세상에 홀로 우뚝 솟아 있다.\n",
      "\n",
      "에메랄드 빛 바다는 속삭이며,\n",
      "해안선이 부드럽게 춤을 춘다.\n",
      "부드러운 모래가 발가락 사이로 미끄러져 내려가고,\n",
      "평화로운 풍경이 마음을 사로잡는다.\n",
      "\n",
      "햇빛이 따뜻하게 비추고,\n",
      "자연의 부드러운 포옹.\n",
      "이 고요한 순간,\n",
      "단순한 기쁨을 찾아서."
     ]
    }
   ],
   "source": [
    "for chunk in gemini.stream(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_uri}}\n",
    "            ]\n",
    "        }\n",
    "    ]):\n",
    "    \n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0664cd19",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## 04-06. 허깅페이스 엔드포인트(HuggingFace Endpoints)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Huggingface Endpoints\n",
    "- Hugging Face Hub은 12만 개 이상의 모델, 2만 개의 데이터셋, 5만 개의 데모 앱(Spaces)을 보유한 플랫폼으로, 모두 오픈 소스이며 공개적으로 사용 가능\n",
    "- Hugging Face Hub은  다양한 ML 애플리케이션을 구축하기 위한 다양한 엔드포인트를 제공\n",
    "    \n",
    "    특히, 텍스트 생성 추론은 Text Generation Inference에 의해 구동. 이는 매우 빠른 텍스트 생성 추론을 위해 맞춤 제작된 Rust, Python, gRPC 서버\n",
    "\n",
    "<br>\n",
    "\n",
    "### 허깅페이스 토큰 발급\n",
    "- 토큰 발급주소: https://huggingface.co/docs/hub/security-tokens\n",
    "\n",
    "<br>\n",
    "\n",
    "### 참고 모델 리스트\n",
    "- 허깅페이스 LLM 리더보드: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "- 모델 리스트: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "- LogicKor 리더보드: https://lk.instruct.kr/\n",
    "\n",
    "<br>\n",
    "\n",
    "```bash\n",
    "pip install -qU huggingface_hub\n",
    "```\n",
    "\n",
    "- `.env` 파일에 발급받은 토큰을 `HUGGINGFACEHUB_API_TOKEN` 을 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e82d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983e5ef",
   "metadata": {},
   "source": [
    "- 허깅페이스 토큰을 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63e5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9150344380424b9aa62147d983ff6116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63341fa9",
   "metadata": {},
   "source": [
    "- 프롬프트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20fa80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125e52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|system|>\n",
    "You are a helpful assistant.<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e493fd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Serverless Endpoints\n",
    "- Inference API 는 무료로 사용할 수 있으며 요금은 제한되어 있습니다. \n",
    "  - 프로덕션을 위한 추론 솔루션이 필요한 경우, Inference Endpoints 서비스를 확인\n",
    "- Inference Endpoints 를 사용하면 모든 머신 러닝 모델을 전용 및 완전 관리형 인프라에 손쉽게 배포할 수 있으며\n",
    "    \n",
    "    클라우드, 지역, 컴퓨팅 인스턴스, 자동 확장 범위 및 보안 수준을 선택하여 모델, 지연 시간, 처리량 및 규정 준수 요구 사항에 맞게 설정 가능\n",
    "    \n",
    "- [Serverless Endpoints](https://huggingface.co/docs/inference-providers/index)\n",
    "- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fb666",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- `repo_id` 변수에 HuggingFace 모델의 repo ID(저장소 ID) 를 할당\n",
    "  - `microsoft/Phi-3-mini-4k-instruct` 모델: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f1e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad142ae3",
   "metadata": {},
   "source": [
    "- 사용할 모델의 저장소 ID를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe92cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"beomi/llama-2-ko-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce8a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 길이를 설정\n",
    "    temperature=0.1,\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # 허깅페이스 토큰\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a3538f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55888f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/beomi/llama-2-ko-7b (Request ID: Root=1-68de3ac4-08ac63b201bd5e2f72e2e893;156f21c0-29f1-4147-9020-355285ba4aad)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/beomi/llama-2-ko-7b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is the capital of South Korea?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3246\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3244\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3245\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3246\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3247\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:392\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    403\u001b[39m         .text\n\u001b[32m    404\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:791\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    788\u001b[39m     **kwargs: Any,\n\u001b[32m    789\u001b[39m ) -> LLMResult:\n\u001b[32m    790\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1002\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    988\u001b[39m     run_managers = [\n\u001b[32m    989\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    990\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1000\u001b[39m         )\n\u001b[32m   1001\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1010\u001b[39m     run_managers = [\n\u001b[32m   1011\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1012\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1019\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1020\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:817\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    807\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    808\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    813\u001b[39m     **kwargs: Any,\n\u001b[32m    814\u001b[39m ) -> LLMResult:\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    816\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    821\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    822\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    824\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    825\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    826\u001b[39m         )\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    828\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1580\u001b[39m, in \u001b[36mLLM._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1577\u001b[39m new_arg_supported = inspect.signature(\u001b[38;5;28mself\u001b[39m._call).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m   1579\u001b[39m     text = (\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1581\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m   1582\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(prompt, stop=stop, **kwargs)\n\u001b[32m   1583\u001b[39m     )\n\u001b[32m   1584\u001b[39m     generations.append([Generation(text=text)])\n\u001b[32m   1585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations=generations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[39m, in \u001b[36mHuggingFaceEndpoint._call\u001b[39m\u001b[34m(self, prompt, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    309\u001b[39m     invocation_params[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m] = invocation_params[\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstop_sequences\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     response_text = json.loads(response.decode())[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[39m, in \u001b[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     warning_message += \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + message\n\u001b[32m    131\u001b[39m warnings.warn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:268\u001b[39m, in \u001b[36mInferenceClient.post\u001b[39m\u001b[34m(self, json, data, model, task, stream)\u001b[39m\n\u001b[32m    266\u001b[39m url = provider_helper._prepare_url(\u001b[38;5;28mself\u001b[39m.token, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    267\u001b[39m headers = provider_helper._prepare_headers(\u001b[38;5;28mself\u001b[39m.headers, \u001b[38;5;28mself\u001b[39m.token)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:321\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\lang_env\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/models/beomi/llama-2-ko-7b (Request ID: Root=1-68de3ac4-08ac63b201bd5e2f72e2e893;156f21c0-29f1-4147-9020-355285ba4aad)"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"what is the capital of South Korea?\"})\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "lang_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
